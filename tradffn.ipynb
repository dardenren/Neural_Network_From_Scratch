{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Traditional feedforward neural network from scratch**\n",
        "\n",
        "This would be my attempt at making a fully-connected multi-layer perceptron(MLP)/feed-forward network from scratch.\n"
      ],
      "metadata": {
        "id": "ayI96eNlMToi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Philosophy of AI**\n",
        "\n",
        "Part of why the realm of AI excites me is the fact that theres many questions pertaining to it and how it intertwines with philosophy. Many thought-provoking questions can be drawn such as \"Can machines think?\" and \"What is it to think?\". An insightful answer to the former question by Hubert Dreyfus: \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zml8adTgDQNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCIN5CS6a2TG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neurons and Layers**\n",
        "* The number of neurons in the input layer is set to be the number of features/dimensions of the data.\n",
        "* There is no framework/general consensus on the number of neurons and layers as it depends on the complexity of the problem. It is usually determined through trial and error.\n",
        "* If the network is tasked to solve a regression problem, the output layer will have a single neuron whereas for classification it will either be single (binary classification) or multiple (multi-class /multi-label classification)\n",
        "\n",
        "\n",
        "\n",
        "**Weight Initialization and Activation Functions**\n",
        "* Xavier initialization is aimed at solving the vanishing gradient problem that arises when using the sigmoid/tanh activation function, so they should be paired together\n",
        "* Weights should not be initialized to be the same as it would lead to equal gradient updates(symmetry problem)\n",
        "* Too high weights would lead to over-saturation, where certain neurons are activated significantly more than their counterparts. This is also why in practice we implement normalization to get more neurons involved in the process of \"estimating\" the problem.\n",
        "\n",
        "\n",
        "**How does training a basic AI work?**\n",
        "\n",
        "1. Data is feed into the neural network and forward pass happens\n",
        "\n",
        "2. Output is compared with expected value to find loss function\n",
        "\n",
        "3. This loss function is used to perform backpropagation to calculate weight_gradients and bias gradients\n",
        "\n",
        "4. These gradients are used to update the weights and parameters to fit the problem(finding the minimun point), with the asssumption of universal approximation theorem.\n",
        "\n",
        "\n",
        "These are the basic idea of training a deep neural network. In reality, there are more complex architecture, optimization algorithms, regularization etc. An example is Recurrent Neural Networks(RNN) which is the common method for problems where discrete data are related to other discrete data such as sentimental analysis, where the word's meaning is affected by its context.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eSLcltOmmvTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward propagation**\n",
        "Forward propagation (forward pass) is the process where the input is fed into the neural network at the input layer, and returns an output at the output layer.\n",
        "\n",
        "For example, we would use a 2-4-7-1 neural network, 2 represents 2 nodes at the input layer which would take two values(data has 2 values such as a coordinate x,y), 1 represents 1 node at the output layer which would return a single value.\n",
        "1.   Input (x,y) is fed into the 2 nodes, where it is then multipled by the weights connected these 2 nodes to the next nodes(4).\n",
        "2. You will get a set of weighted sum(wsum) where it corresponds to the 4 nodes, so you would get 4 (wsum).\n",
        "3. These wsum are then added with the biases element-wise and then fed into the activation function to get the \"activations\". These activations can been seen as the input to the 4 nodes.  \n",
        "4.   The same process happens up till the output layer where the activations will be the output value\n",
        "\n",
        "However, is it paramount to note that in practice, there are tweaks such as not initializing bias for output layer or not initializing bias at all!\n",
        "\n",
        "Some neurons might not be connected to all the proceeding neurons, such as for a 2-4-7-1, each of the 2 neurons at input layer might be connected to 3 of the 4 neurons. (Sparse layer)\n",
        "\n",
        "These are done in consideration of many factors, such as overfitting vs underfitting, making use of all neurons such as \"dropout\" layers, etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTDwpI8ECHC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward propagation**\n",
        "Backward propagation is the process where the \"training\" of the neural network takes place\n",
        "\n",
        "1. The output from forward propagation is compared with the expected value to get a loss value.\n",
        "2. Using this loss value, the back propagation algorithm calculate weight gradients and bias gradients, which basically calculates the change in parameters values to \"model\" a more accurate model.\n",
        "3. Technically, it is usually the optimizer(if there is one) that adjusts these parameters, but back propagation is commonly associated to the updating of the parameters.  \n",
        "4. Repeat this cycle (epoch)\n",
        "\n",
        "\n",
        "Fun fact: Back propagation makes use of partial differentiation\n",
        "\n"
      ],
      "metadata": {
        "id": "NblD7FSDCQvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.layers = [] # keep track of number of nodes per layer (easier for params init)\n",
        "    self.activation_functions = {}\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.neuron_values = []\n",
        "    self.metric = None\n",
        "\n",
        "  def model_summary(self):\n",
        "    if not self.layers:\n",
        "      raise InvalidModel('There is no layers added.')\n",
        "\n",
        "    if not self.biases:\n",
        "      self.bias_init()\n",
        "\n",
        "    if not self.weights:\n",
        "      self.weight_init()\n",
        "\n",
        "    total_params = 0\n",
        "    print(f'Input layer  : {self.layers[0]} nodes')\n",
        "    if self.activation_functions.get(1):\n",
        "        print(f'Activation Function: {self.activation_functions.get(1)}')\n",
        "\n",
        "    for i in range(1,len(self.layers)-1):\n",
        "      if self.biases:\n",
        "        param = self.layers[i] * self.layers[i-1] + self.layers[i]\n",
        "      else:\n",
        "        param = self.layers[i] * self.layers[i-1]\n",
        "      print(f'Hidden layer{i}: {self.layers[i]} nodes|Number of parameters: {param}')\n",
        "      total_params += param\n",
        "      if self.activation_functions.get(i):\n",
        "        print(f'Activation Function: {self.activation_functions.get(i)}')\n",
        "\n",
        "    if self.biases:\n",
        "        param = self.layers[-1] * self.layers[-2] + self.layers[-1]\n",
        "    else:\n",
        "        param = self.layers[-1] * self.layers[-2]\n",
        "    print(f'Output layer : {self.layers[-1]} nodes|Number of parameters: {param}')\n",
        "    total_params += param\n",
        "    print(f'Total trainable parameters: {total_params}')\n",
        "\n",
        "\n",
        "  # Setup of network\n",
        "  def add_layer(self,nodes=0,layer='Dense',activation=None):\n",
        "    if activation:\n",
        "      self.activation_functions[len(self.layers)] = activation\n",
        "    else:\n",
        "      self.layers.append(nodes)\n",
        "\n",
        "\n",
        "\n",
        "  def weight_init(self,init='he',factor=None,gain=1,seed=0):\n",
        "    if self.weights:\n",
        "      self.weights = []\n",
        "    if len(self.layers) <= 2:\n",
        "      raise LayerCountError(f'Number of layers = {self.layer_count} layers, requried to add more. ')\n",
        "\n",
        "    if init == 'xavier':\n",
        "      factor = 6\n",
        "\n",
        "    if init == 'he':\n",
        "      factor = 2\n",
        "\n",
        "    if init == 'custom':\n",
        "      factor = factor\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    for i in range(0,len(self.layers) - 1):\n",
        "      input_neuron = self.layers[i] #number of input neurons\n",
        "      output_neuron = self.layers[i+1] #number of the next layer neurons\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale = gain * np.sqrt(factor/(input_neuron + output_neuron)),\n",
        "                                          size = (input_neuron,output_neuron)))\n",
        "\n",
        "    return self.weights\n",
        "\n",
        "\n",
        "  def bias_init(self,value=0.01,init=True,method='constant',mean=0,var=0.01):\n",
        "    if self.biases:\n",
        "      self.biases = []\n",
        "    if method == 'constant':\n",
        "      bias = 0.01 #0.01 is a widely used and tested value for bias initialization\n",
        "\n",
        "    if method == 'gaussian':\n",
        "      bias = np.random.normal(loc=0.0,scale=np.sqrt(0.01))\n",
        "\n",
        "    self.biases = [np.random.normal(loc=0.0,scale=np.sqrt(0.01),size=(i,1)) for i in self.layers[1:]]\n",
        "\n",
        "    # for i in range(0,len(self.layers) - 1):\n",
        "    #   input_neuron = self.layers[i]\n",
        "    #   output_neuron = self.layers[i+1]\n",
        "    #   self.biases.append(np.full((output_neuron,),bias,dtype=float))\n",
        "\n",
        "    return self.biases\n",
        "\n",
        "\n",
        "\n",
        "                                             #   first bias is 5x1\n",
        "  def forward(self,X,batch=True): #X is input    3-5-2-1   first weight is 5x3, input 1x3\n",
        "    if not self.weights:\n",
        "      self.weights_init()\n",
        "\n",
        "    if not self.biases:\n",
        "      self.bias_init()\n",
        "\n",
        "    batch_wsum = []\n",
        "    batch_activations = []\n",
        "    outputs = []\n",
        "\n",
        "    if not batch:\n",
        "      X = np.expand_dims(X.copy(), axis=0)\n",
        "\n",
        "    for x in X:\n",
        "      values = np.transpose(x.copy())\n",
        "      weighted_sum_list = []\n",
        "      activation_list = []\n",
        "      for i in range(len(self.layers)-1):\n",
        "        if self.activation_functions.get(i):\n",
        "          if self.activation_functions[i] == 'relu':\n",
        "            func = lambda x: max(0,x)\n",
        "\n",
        "          elif self.activation_functions[i] == 'sigmoid':\n",
        "            func = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "          elif self.activation_functions[i] == 'tanh':\n",
        "            func = lambda x:(np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "\n",
        "          vectorizer = np.vectorize(func)\n",
        "\n",
        "          weighted_sum = np.sum(np.dot(values,self.weights[i]) + self.biases[i],axis=0,keepdims=True)\n",
        "          values = vectorizer(weighted_sum)\n",
        "\n",
        "        else:\n",
        "          weighted_sum = np.sum(np.dot(values,self.weights[i]) + self.biases[i],axis=0,keepdims=True)\n",
        "          values = weighted_sum\n",
        "\n",
        "\n",
        "\n",
        "        weighted_sum_list.append(weighted_sum)\n",
        "        activation_list.append(values)\n",
        "\n",
        "      outputs.append(values)\n",
        "      batch_wsum.append(weighted_sum_list)\n",
        "      batch_activations.append(activation_list)\n",
        "\n",
        "      # Flatten outputs\n",
        "      outputs = [array.flatten() for array in outputs]\n",
        "\n",
        "\n",
        "    return outputs,batch_wsum,batch_activations\n",
        "\n",
        "\n",
        "    # else:\n",
        "    #   values = np.transpose(X.copy())\n",
        "    #   for i in range(len(self.layers)-1):\n",
        "    #     if self.activation_functions.get(i):\n",
        "    #       if self.activation_functions[i] == 'relu':\n",
        "    #         func = lambda x: max(0,x)\n",
        "\n",
        "    #       if self.activation_functions[i] == 'sigmoid':\n",
        "    #         func = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "    #       if self.activation_functions[i] == 'tanh':\n",
        "    #         func = lambda x:(np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "    #       vectorizer = np.vectorize(func)\n",
        "\n",
        "    #       weighted_sum = np.dot(values,self.weights[i]) + self.biases[i]\n",
        "    #       print(f'weighted_sum = {weighted_sum}')\n",
        "    #       values = vectorizer(weighted_sum,signature='()->(n)')\n",
        "    #       print(f'values = {values}')\n",
        "\n",
        "    #     else:\n",
        "    #       print('no')\n",
        "    #       weighted_sum = np.dot(values,self.weights[i]) + self.biases[i]\n",
        "    #       values = weighted_sum\n",
        "\n",
        "    #     weighted_sum_list.append(weighted_sum)\n",
        "    #     activation_list.append(values)\n",
        "\n",
        "    # return values,weighted_sum_list,activation_list\n",
        "\n",
        "\n",
        "  # Prediction\n",
        "  def predict(self,x):\n",
        "    outputs,_,_ = self.forward(x)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  # Loss function & accuracy metrics\n",
        "\n",
        "  def mse_error(self,target,predicted):\n",
        "    assert len(target) == len(predicted), \"target shape is not equal to predicted shape.\"\n",
        "    # if not isinstance(target,np.ndarray):\n",
        "    #   target = np.array(target)\n",
        "    # if not isinstance(predicted,np.ndarray):\n",
        "    #   predicted = np.array(predicted)\n",
        "\n",
        "    # if len(target.shape) == 1 or target.shape[0] != 1:\n",
        "    #   x = target.shape[0]\n",
        "    # else:\n",
        "    #   x = target.shape[1]\n",
        "\n",
        "    return np.sum((target-predicted)**2)/len(target)\n",
        "\n",
        "\n",
        "  def mse_error_grad(self,target,predicted):\n",
        "    return 2*(predicted-target)\n",
        "\n",
        "\n",
        "  def loss(self,target,predicted,method='MSELoss',batch=False):\n",
        "    # pred is output of self.forward(), target is actual labels values\n",
        "    if batch:\n",
        "      losses = []\n",
        "      for tar,pre in zip(target,predicted):\n",
        "        if method == 'MSELoss':\n",
        "          losses.append([self.mse_error(tar,pre),self.mse_error_grad(tar,pre)])\n",
        "      return losses\n",
        "\n",
        "\n",
        "    else:\n",
        "      if method == 'MSELoss':\n",
        "        if type(target) == list:\n",
        "          target = target[0]\n",
        "        if type(predicted) == list:\n",
        "          predicted = predicted[0]\n",
        "        return self.mse_error(target,predicted),self.mse_error_grad(target,predicted)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Derivatives of activation functions\n",
        "  def relu_grad(x):\n",
        "    return max(0,x)\n",
        "\n",
        "  def sigmoid_grad(x):\n",
        "    return (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x)))\n",
        "\n",
        "  def tanh_grad(x):\n",
        "    pass\n",
        "\n",
        "\n",
        "  # Updating parameters\n",
        "  def backward(self,x,y,batch=1):\n",
        "    if not self.weights:\n",
        "      self.weight_init()\n",
        "\n",
        "    if not self.biases:\n",
        "      self.bias_init()\n",
        "\n",
        "    # assert x.shape[0] == y.shape[0]\n",
        "    # Keep track of gradients\n",
        "    weight_gradients = [np.zeros(w.shape) for w in self.weights]\n",
        "    bias_gradients = [np.zeros((b.shape[0],1)) for b in self.biases]\n",
        "\n",
        "    # Batches\n",
        "    left,right = 0,batch\n",
        "    while left < x.shape[0]:\n",
        "      if left >= x.shape[0]:\n",
        "        break\n",
        "      if right == -1:\n",
        "        X_train_batch = x[left:]\n",
        "        y_train_batch = y[left:]\n",
        "      else:\n",
        "        X_train_batch = x[left:right]\n",
        "        y_train_batch = y[left:right]\n",
        "\n",
        "      for datapoint in range(batch):\n",
        "        if batch > 1 and left !=len(x)-1:\n",
        "          output,wsum_list,activation_list = self.forward(X_train_batch,batch=True)\n",
        "          loss,loss_grad = self.loss(y_train_batch,output,method='MSELoss',batch=True)\n",
        "          dc_da = loss_grad[datapoint]\n",
        "        else:\n",
        "          output,wsum_list,activation_list = self.forward(X_train_batch[0],batch=False)\n",
        "          loss,loss_grad = self.loss(y_train_batch,output,method='MSELoss')\n",
        "          dc_da = loss_grad\n",
        "\n",
        "        for i in range(len(self.layers)-1,-1,-1):\n",
        "          if self.activation_functions.get(i):\n",
        "            grad_name = f'{self.activation_functions[i]}_grad'\n",
        "            activation_grad_func = getattr(NeuralNetwork,grad_name) # becomes self.relu_grad()\n",
        "            activation_grad_func = np.vectorize(activation_grad_func)\n",
        "          else:\n",
        "            activation_grad_func = None\n",
        "\n",
        "          # Output layer\n",
        "          if i == (len(self.layers) - 1):\n",
        "            weighted_sum = wsum_list[0][i-1]\n",
        "            activation = activation_list[0][i-1]\n",
        "            if activation_grad_func:\n",
        "              local_gradient = dc_da * activation_grad_func(weighted_sum)\n",
        "              print(f\"Output Layer Local Gradient = {local_gradient}\")\n",
        "            else:\n",
        "              local_gradient = np.full(weighted_sum.shape,dc_da)\n",
        "              # local_gradient = dc_da\n",
        "            # print(local_gradient.shape)\n",
        "            # weight_loss_gradient = local_gradient * activation\n",
        "            bias_loss_gradient = local_gradient\n",
        "            bias_gradients[i-1] += bias_loss_gradient\n",
        "\n",
        "          # Input layer\n",
        "          elif i == 0:\n",
        "            weighted_sum = wsum_list[0][0]\n",
        "            activation = activation_list[0][0]\n",
        "            weights = self.weights[0]\n",
        "\n",
        "            if self.activation_functions.get(i):\n",
        "              local_gradient = np.multiply(np.transpose(activation_grad_func(weighted_sum)),(np.dot(weights, local_gradient)))\n",
        "            else:\n",
        "              local_gradient = np.dot(weights, local_gradient)\n",
        "\n",
        "            #2x1, 4x1\n",
        "            weight_loss_gradient = local_gradient * activation\n",
        "            weight_gradients[0] += weight_loss_gradient\n",
        "\n",
        "          # Hidden layers\n",
        "          else:\n",
        "            #local gradient is cached previously\n",
        "            weighted_sum = wsum_list[0][i-1] #1x7\n",
        "            activation = activation_list[0][i-1] #1x7\n",
        "            weights = self.weights[i] #7x1\n",
        "\n",
        "            if self.activation_functions.get(i):\n",
        "              local_gradient = np.multiply(np.transpose(activation_grad_func(weighted_sum)),(np.dot(weights, local_gradient))) #7x1\n",
        "            else:\n",
        "              local_gradient = np.dot(weights, local_gradient)\n",
        "\n",
        "            weight_loss_gradient = local_gradient * np.transpose(activation)\n",
        "            bias_loss_gradient = local_gradient\n",
        "\n",
        "\n",
        "            # Edit gradients in place\n",
        "            weight_gradients[i] += weight_loss_gradient\n",
        "            bias_gradients[i-1] += bias_loss_gradient\n",
        "\n",
        "\n",
        "\n",
        "        # Update parameters\n",
        "\n",
        "\n",
        "      left += batch\n",
        "      right += batch\n",
        "      if right > x.shape[0]:\n",
        "        right = x.shape[0]\n",
        "\n",
        "    return weight_gradients, bias_gradients\n",
        "\n",
        "\n",
        "\n",
        "  # Define how backprop is used\n",
        "  def optimize(self,method='SGD'):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def update_parameters(self,weight_grad,bias_grad,lr=0.005):\n",
        "    for i in range(len(weight_grad)):\n",
        "      print(weight_grad[i])\n",
        "      print(bias_grad[i])\n",
        "      self.weights[i] -= lr * weight_grad[i]\n",
        "      # self.biases[i] -= lr * bias_grad[i]\n",
        "    return\n",
        "\n",
        "\n",
        "  # Function to train model (Compilation of main functions)\n",
        "  def fit(self,X,y,lr=0.005,batch=1):\n",
        "    weight_gradients, bias_gradients = self.backward(X,y,batch=1)\n",
        "    self.update_parameters(weight_gradients, bias_gradients, lr=0.005)\n",
        "    return\n",
        "\n",
        "\n",
        "  def accuracy(self,X,y):\n",
        "    predicted = self.predict(X)\n",
        "    if self.metric == 'MSE':\n",
        "      error = ((predicted - np.array(X))**2).mean(ax=1)\n",
        "    return error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I_levPEFKZft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_grad,b_grad,l_grad = model.backward(input,output)\n",
        "w_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRk5zPazrNYr",
        "outputId": "2b769e5d-e28d-48c3-e70e-e3b50c291b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-7.12842285e-45,  1.79976900e-45, -3.74386883e-45,\n",
              "         -2.90507155e-45],\n",
              "        [ 1.74081550e-44, -4.39517386e-45,  9.14281466e-45,\n",
              "          7.09440740e-45]]),\n",
              " array([[-7.56073940e-85, -7.56073940e-85, -7.56073940e-85,\n",
              "         -7.56073940e-85, -7.56073940e-85, -7.56073940e-85,\n",
              "         -7.56073940e-85],\n",
              "        [ 4.49735492e-45,  4.49735492e-45,  4.49735492e-45,\n",
              "          4.49735492e-45,  4.49735492e-45,  4.49735492e-45,\n",
              "          4.49735492e-45],\n",
              "        [ 5.48685762e-52,  5.48685762e-52,  5.48685762e-52,\n",
              "          5.48685762e-52,  5.48685762e-52,  5.48685762e-52,\n",
              "          5.48685762e-52],\n",
              "        [-8.66316749e-50, -8.66316749e-50, -8.66316749e-50,\n",
              "         -8.66316749e-50, -8.66316749e-50, -8.66316749e-50,\n",
              "         -8.66316749e-50]]),\n",
              " array([[-1.62134948e-231],\n",
              "        [-1.57742757e-078],\n",
              "        [ 0.00000000e+000],\n",
              "        [ 1.43486832e-108],\n",
              "        [ 1.42487208e-144],\n",
              "        [ 1.54878612e-231],\n",
              "        [ 0.00000000e+000]])]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetted = np.array([\n",
        "                  [4],\n",
        "                  [2],\n",
        "                  [5],\n",
        "])\n",
        "\n",
        "predicted = np.array([\n",
        "                  [2],\n",
        "                  [6],\n",
        "                  [4],\n",
        "])\n",
        "\n",
        "predicted_2 = np.array([\n",
        "    [0.49495892],\n",
        "    [0.50364126],\n",
        "    [0.44763302]\n",
        "  ])\n",
        "\n",
        "model.mse_error(targetted,predicted_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80oZ8-F83UC1",
        "outputId": "d60c8020-ab41-4b59-e0d4-7ee73189f549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =[[3.50504108]\n",
            " [1.49635874]\n",
            " [4.55236698]]\n",
            "sum=35.248447571856275\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.749482523952091"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first = np.array([1,3,5])\n",
        "second = np.array([4,5,2])\n",
        "first*second"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IltcH7kd130_",
        "outputId": "f2dc034c-3135-44fd-9ad4-d240a80007eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4, 15, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output,_,_ = model.forward(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DckURb76kzZ",
        "outputId": "fe703e05-6b10-4061-8fa8-b26f585bca18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-c487c5891668>:120: RuntimeWarning: overflow encountered in exp\n",
            "  func = lambda x: 1/(1+np.exp(-x))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.49207497]]), array([[0.50077704]]), array([[0.44480438]])]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()\n",
        "model.add_layer(2)\n",
        "model.add_layer(activation='sigmoid')\n",
        "model.add_layer(4)\n",
        "model.add_layer(activation='sigmoid')\n",
        "model.add_layer(7)\n",
        "model.add_layer(activation='sigmoid')\n",
        "model.add_layer(1)\n",
        "model.model_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcvPGgOd56cj",
        "outputId": "97d72b23-e502-4744-aa57-cf7483543a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input layer  : 2 nodes\n",
            "Activation Function: sigmoid\n",
            "Hidden layer1: 4 nodes|Number of parameters: 12\n",
            "Activation Function: sigmoid\n",
            "Hidden layer2: 7 nodes|Number of parameters: 35\n",
            "Activation Function: sigmoid\n",
            "Output layer : 1 nodes|Number of parameters: 8\n",
            "Total trainable parameters: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weight_init(init='xavier',seed=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpbaJJ2Rj7xR",
        "outputId": "ace56ade-9eb7-453c-e08c-bdd46d26a793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ],\n",
              "        [ 1.86755799, -0.97727788,  0.95008842, -0.15135721]]),\n",
              " array([[-0.07623217,  0.30324709,  0.10638323,  1.07405217,  0.56206361,\n",
              "          0.08986296,  0.32781472],\n",
              "        [ 0.24643482,  1.10345052, -0.15151942,  0.23121582, -0.63079151,\n",
              "         -1.88550794,  0.48272932],\n",
              "        [ 0.63842844, -0.54812519,  1.67632488, -1.07412024,  0.0337949 ,\n",
              "         -0.13824444,  1.13203247],\n",
              "        [ 1.08519337,  0.11443626,  0.27929153, -0.65567323, -1.46291514,\n",
              "         -0.25695015,  0.11547137]]),\n",
              " array([[ 1.06546298],\n",
              "        [ 1.04129149],\n",
              "        [-0.33543486],\n",
              "        [-0.26180186],\n",
              "        [-0.9080735 ],\n",
              "        [-1.22977161],\n",
              "        [-1.47767333]])]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input = np.array([\n",
        "#          [[-2,-5,-3],[-5,-7,-4]],\n",
        "#          [[5,2,1],[5,8,2]],\n",
        "#          [[12,53,72],[32,23,91]],\n",
        "#          ])   #3 x 2x3\n",
        "\n",
        "input = np.array([\n",
        "                  [[3],[5]],\n",
        "                  [[1],[9]],\n",
        "                  [[-3],[-7]],  #3 x 2\n",
        "])\n",
        "\n",
        "output = np.array([\n",
        "                  [4],\n",
        "                  [2],\n",
        "                  [5],\n",
        "])\n"
      ],
      "metadata": {
        "id": "S5xgae2880NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.biases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brb3LGwakmRH",
        "outputId": "c56c4d9c-826d-49b3-ae0a-dd53095473c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRl-ZIFolgub",
        "outputId": "26a55e98-31dd-4c36-f62c-e1c5e57348ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ],\n",
              "        [ 1.86755799, -0.97727788,  0.95008842, -0.15135721]]),\n",
              " array([[-0.07623217,  0.30324709,  0.10638323,  1.07405217,  0.56206361,\n",
              "          0.08986296,  0.32781472],\n",
              "        [ 0.24643482,  1.10345052, -0.15151942,  0.23121582, -0.63079151,\n",
              "         -1.88550794,  0.48272932],\n",
              "        [ 0.63842844, -0.54812519,  1.67632488, -1.07412024,  0.0337949 ,\n",
              "         -0.13824444,  1.13203247],\n",
              "        [ 1.08519337,  0.11443626,  0.27929153, -0.65567323, -1.46291514,\n",
              "         -0.25695015,  0.11547137]]),\n",
              " array([[ 1.06546298],\n",
              "        [ 1.04129149],\n",
              "        [-0.33543486],\n",
              "        [-0.26180186],\n",
              "        [-0.9080735 ],\n",
              "        [-1.22977161],\n",
              "        [-1.47767333]])]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "learning_rate = 0.01\n",
        "batch = 3\n",
        "epochs = 100\n",
        "\n",
        "model.fit(input,output,batch=batch,lr=learning_rate)\n",
        "pred = model.predict(input)\n",
        "\n",
        "\n",
        "# for i in tqdm(range(epochs)):\n",
        "#   model.fit(input,output,batch=batch,lr=learning_rate)\n",
        "#   pred = model.predict(input)\n",
        "#   print(pred)\n",
        "#   print(f\"Epoch {i+1}: Loss = {model.mse_error(output,pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOI5NbE1u7r0",
        "outputId": "34ebdb97-eaa4-44cb-ec16-ded553ea37b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Layer Local Gradient = [[-0.92653331]]\n",
            "Output Layer Local Gradient = [[-0.44653327]]\n",
            "Output Layer Local Gradient = [[-1.99352511]]\n",
            "[[-1.28044078e-12  3.25115225e-13 -6.72219865e-13 -5.19186134e-13]\n",
            " [ 2.91182473e-12 -7.36401425e-13  1.52886125e-12  1.18596488e-12]]\n",
            "[[ 1.54372760e-24]\n",
            " [-1.52535171e-13]\n",
            " [ 8.07529347e-19]\n",
            " [-2.04392944e-15]]\n",
            "[[ 5.21184677e-23  5.21184677e-23  5.21184677e-23  5.21184677e-23\n",
            "   5.21184677e-23  5.21184677e-23  5.21184677e-23]\n",
            " [ 1.30222893e-12  1.30222893e-12  1.30222893e-12  1.30222893e-12\n",
            "   1.30222893e-12  1.30222893e-12  1.30222893e-12]\n",
            " [ 1.43147466e-17  1.43147466e-17  1.43147466e-17  1.43147466e-17\n",
            "   1.43147466e-17  1.43147466e-17  1.43147466e-17]\n",
            " [-2.66999423e-14 -2.66999423e-14 -2.66999423e-14 -2.66999423e-14\n",
            "  -2.66999423e-14 -2.66999423e-14 -2.66999423e-14]]\n",
            "[[-1.82975540e-15]\n",
            " [-5.59957172e-14]\n",
            " [ 2.66748022e-84]\n",
            " [ 2.25354219e-12]\n",
            " [ 2.13814877e-09]\n",
            " [ 3.78476238e-39]\n",
            " [ 9.82485746e-61]]\n",
            "[[-1.82975540e-015]\n",
            " [-5.62907315e-027]\n",
            " [ 1.84303309e-167]\n",
            " [ 2.25354188e-012]\n",
            " [ 2.13814876e-009]\n",
            " [ 1.01202605e-077]\n",
            " [ 5.67561592e-121]]\n",
            "[[-3.36659169]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forwarded = model.forward(input)\n",
        "output,wsum_list,_ = forwarded\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0959edZASYjE",
        "outputId": "cf4c5c69-44ea-4c8d-ac3e-8c28f4c21fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.2923489]]), array([[0.2923489]]), array([[0.62195888]])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsum_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5n0vbL7GZrl",
        "outputId": "2f26acf2-1f07-4895-a091-581a2d20c336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 58.49481331, -14.76864574,  30.72164951,  23.83859958]]),\n",
              " array([[176.84045068, -60.36547531, 315.37922365,  50.77434953,\n",
              "          39.28444625, 107.39537082, 234.62057112]]),\n",
              " array([[-1.04115536]])]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_grad,b_grad,loc = model.backward(input,output,batch=2)"
      ],
      "metadata": {
        "id": "yyXCqGcjkeu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c205a65-ad9a-4f68-d265-444148393bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, array([[0.40552918]])]\n",
            "[0, array([[0.40552918]])]\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-bfe2510f43af>:120: RuntimeWarning: overflow encountered in exp\n",
            "  func = lambda x: 1/(1+np.exp(-x))\n",
            "<ipython-input-38-bfe2510f43af>:228: RuntimeWarning: overflow encountered in exp\n",
            "  return (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjR_iRUN7bIn",
        "outputId": "5f3a5c5d-c929-4e85-bc00-270399ddbe4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-7.71349165e-50],\n",
              "       [-9.10586327e-51]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOEgl39eUWjH",
        "outputId": "e2650247-5fc9-4307-f722-1890dbab2678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.70540250e-34,  4.30576386e-35, -8.95682452e-35,\n",
              "         -6.95008753e-35],\n",
              "        [ 4.16472362e-34, -1.05150054e-34,  2.18732520e-34,\n",
              "          1.69726465e-34]]),\n",
              " array([[-7.68505140e-70, -7.68505140e-70, -7.68505140e-70,\n",
              "         -7.68505140e-70, -7.68505140e-70, -7.68505140e-70,\n",
              "         -7.68505140e-70],\n",
              "        [ 1.07594632e-34,  1.07594632e-34,  1.07594632e-34,\n",
              "          1.07594632e-34,  1.07594632e-34,  1.07594632e-34,\n",
              "          1.07594632e-34],\n",
              "        [ 1.31267475e-41,  1.31267475e-41,  1.31267475e-41,\n",
              "          1.31267475e-41,  1.31267475e-41,  1.31267475e-41,\n",
              "          1.31267475e-41],\n",
              "        [-2.07257450e-39, -2.07257450e-39, -2.07257450e-39,\n",
              "         -2.07257450e-39, -2.07257450e-39, -2.07257450e-39,\n",
              "         -2.07257450e-39]]),\n",
              " array([[-4.04195148e-159],\n",
              "        [-1.53693395e-055],\n",
              "        [ 0.00000000e+000],\n",
              "        [ 4.41745462e-076],\n",
              "        [ 2.78912225e-100],\n",
              "        [ 4.10437439e-159],\n",
              "        [ 1.01675607e-246]])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4ygfEa6V17f",
        "outputId": "e92ca371-a61b-4e53-c6ce-9521182158c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.04579543e-71],\n",
              "        [-7.28534177e-36],\n",
              "        [ 4.27280035e-43],\n",
              "        [-8.69419574e-41]]),\n",
              " array([[-7.20362902e-081],\n",
              "        [-2.52957773e-029],\n",
              "        [ 8.39540515e-172],\n",
              "        [ 1.18048105e-039],\n",
              "        [ 1.74695165e-051],\n",
              "        [ 7.79870197e-081],\n",
              "        [ 1.34550158e-124]]),\n",
              " array([[-0.04864247]])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00DL_L5Iqwl4",
        "outputId": "81b3e0dc-ea09-4c4e-a5a7-7895490da723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ],\n",
              "        [ 1.86755799, -0.97727788,  0.95008842, -0.15135721]]),\n",
              " array([[-0.05160943,  0.20529925,  0.07202179,  0.72713675,  0.38051886,\n",
              "          0.06083751,  0.22193162],\n",
              "        [ 0.16683716,  0.74703954, -0.10257913,  0.15653385, -0.42704787,\n",
              "         -1.27649491,  0.3268093 ],\n",
              "        [ 0.4322181 , -0.37108251,  1.13487731, -0.72718284,  0.02287926,\n",
              "         -0.09359193,  0.76638961],\n",
              "        [ 0.73467938,  0.07747371,  0.18908126, -0.44389287, -0.99039823,\n",
              "         -0.17395607,  0.07817448]]),\n",
              " array([[ 0.35151162],\n",
              "        [ 0.3435371 ],\n",
              "        [-0.1106648 ],\n",
              "        [-0.08637221],\n",
              "        [-0.29958656],\n",
              "        [-0.40571941],\n",
              "        [-0.48750577]])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build network architecture\n",
        "model = NeuralNetwork()\n",
        "\n",
        "\n",
        "batch_size = X_train.shape[0]\n",
        "epochs = 100\n",
        "backprop_batch = 10\n",
        "lr = 1e-17  #If batch-size is increased, learning rate is decreased\n",
        "\n",
        "\n",
        "# Training(forward and backward pass)\n",
        "for i in tqdm(range(epochs)):\n",
        "  left,right = 0,backprop_batch\n",
        "  while left < batch_size:\n",
        "    X_train_batch = X_train[left:right]\n",
        "    y_train_batch = y_train[left:right]\n",
        "\n",
        "    # To be replaced with backprop function\n",
        "    weight_gradients = [np.zeros(w.shape) for w in self.weights]\n",
        "    bias_gradients = [np.zeros(b.shape) for b in self.biases]\n",
        "    for i in range(backprop_batch):\n",
        "      output,activation_list = model.forward(X_train_batch[i])\n",
        "      loss = model.loss(y_train_batch[i],output,method='MSELoss')\n",
        "\n",
        "      # Update parameter gradients\n",
        "\n",
        "    left += backprop_batch\n",
        "    right += backprop_batch\n",
        "    if right > batch_size:\n",
        "      right = batch_size+1\n",
        "\n",
        "\n",
        "  if (epoch+1) == 10:\n",
        "    print(f'Epoch: {i}, Loss: {loss}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2wSErUIMr9Zy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c735ca5e-bef0-4001-a405-ad40304b267b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-c4a9b2c750a6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbackprop_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a 2-4-7-1 model\n",
        "model = NeuralNetwork()\n",
        "model.add_layer(2)\n",
        "model.add_layer(4)\n",
        "model.add_layer(7)\n",
        "model.add_layer(1)\n",
        "model.model_summary()\n",
        "\n",
        "# model2 = NeuralNetwork()\n",
        "# model2.add_layer(2)\n",
        "# model2.add_layer(4)\n",
        "# model2.add_layer(7)\n",
        "# model2.add_layer(1)\n",
        "# model2.bias_init()\n",
        "# print(model2.biases)\n",
        "# model2.model_summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "ElodA9KCcmu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concepts I learnt from this project\n",
        "\n",
        "1) For mini-batch stochastic gradient descent, within a batch, the local gradients are accumulated and are used to update the parameters. Hence there is an inverse relationship between the batch-size and the learning rate (e.g. the higher the batch-size, the higher the accumulated local gradients and thus learning rate should be lower to compensate)"
      ],
      "metadata": {
        "id": "Fm-B2rhQ4zaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "Kumar, S. K. (2017). On weight initialization in deep neural networks. Retrieved from http://arxiv.org/abs/1704.08863\n",
        "\n",
        "Philosophy of artifical intelligence.Retrieved from https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#:~:text=The%20philosophy%20of%20artificial%20intelligence,%2C%20epistemology%2C%20and%20free%20will."
      ],
      "metadata": {
        "id": "Yhykw7i8j0NX"
      }
    }
  ]
}